二、 核心方法论与设计理念
2.1 独立双层模型体系 (Independent Dual-Layer Architecture)
平台遵循“语义—统计”解耦的双层建模思想，但对每个数据集 $$D_i$$ 独立实例化：
- 第一层：语义嵌入层 (Semantic Layer)
  - 模型：针对数据集 $D_i$ 独立微调的 Qwen 模型（Encoder-only 模式或 Pooling 提取）。
  - 目标：将非结构化文本映射为稠密向量，捕捉该领域特有的语义细微差别（如在煤炭语境下，“Transition”特指能源转型而非普通改变）。
  - 输出：文档嵌入矩阵 $X \in \mathbb{R}^{N \times 1024}$。
- 第二层：主题生成层 (Generative Layer)
  - 模型：ETM (Embedded Topic Model)，基于 VAE 架构。
  - 目标：在语义向量空间中发现潜在的主题结构。
  - 机制：使用摊销推断（Amortized Inference）代替 Gibbs 采样，支持端到端梯度下降。
2.2 语义—结构连接机制 (核心创新)
本项目的核心在于利用 Qwen 的语义知识初始化 ETM 的解码器，公式如下：
$$P(w | z) \propto \exp( \rho^T \alpha_z )$$
其中：
- $$\rho \in \mathbb{R}^{V \times 1024}$$：词向量矩阵。这是将该数据集的 Top-K 词表输入微调后的 Qwen 得到的，作为 ETM 的语义锚点。
- $$\alpha_z \in \mathbb{R}^{1024}$$：主题向量。模型学习到的主题在语义空间中的方向。
- 优势：主题不再是词的随机共现，而是语义空间中的几何聚类。
2.3 多语言处理策略
- 放弃强制的“多语言统一空间”联合训练。
- 利用 Qwen 基座强大的多语言能力，分别处理英语和德语数据集。
- 优势：避免了将不同语言强行映射到同一空间时造成的语义精度损失（Semantic Dilution）。

---
三、 硬件与运行环境
3.1 硬件配置
- GPU：NVIDIA RTX 4090 D (24GB 显存) —— 单卡即可满足单数据集流水线需求。
- 存储：系统盘 30GB + 数据盘 50GB。
3.2 软件环境
- 核心框架：Python 3.12, PyTorch 2.x
- LLM 工具：HuggingFace Transformers, PEFT (用于 LoRA 微调)
- 科学计算：NumPy, SciPy
- 可视化：Matplotlib, Seaborn, PyLDAvis

---
四、 数据集概况
- 数据隔离：每个数据集视为一个独立的工程对象。
- 输入格式：纯文本列表（List of Strings），无需预先标注（除非进行有监督微调）。
- 规模：总计约 170 万条，单数据集规模从数万到数十万不等。

---
五、 完整技术流程（端到端独立流水线）
针对任意一个数据集（假设名为 Dataset_A），执行以下标准流程：
阶段 1：Qwen 模型微调 (Engine B - Training)
- 输入：Dataset_A 的原始文本。
- 方法：
  - 无监督 (推荐)：SimCSE (对比学习)。构建 (原句, Dropout句) 正样本对，拉近自身距离，推远其他句子。
  - 微调技术：LoRA (Low-Rank Adaptation)，仅训练约 0.1% 的参数，显存占用低。
- 输出：Qwen_A_LoRA 权重文件。
阶段 2：语义矩阵生成 (Engine B - Inference)
- 操作：加载 Qwen_A_LoRA，冻结参数。
- 动作 A (文档编码)：将 $N$ 篇文档输入模型，取 Last Hidden State 的 Mean Pooling。
  - 产出：文档嵌入矩阵 $X$ (Shape: $N \times 1024$)。
- 动作 B (词表编码)：构建该数据集的 Top-V 词表（如 5000 词），输入模型。
  - 产出：词向量矩阵 $\rho$ (Shape: $V \times 1024$)。
阶段 3：结构化数据构建 (Engine A)
- 操作：传统的 NLP 预处理（分词、去停用词）。
- 产出：BOW (词袋) 矩阵 (Shape: $N \times V$)。
  - 注意：这是 ETM Decoder 的重构目标 (Ground Truth)。
阶段 4：ETM 模型训练 (Engine C - VAE Training)
这是核心数学过程，摒弃 Gibbs 采样。
- 输入：文档嵌入矩阵 $X$，BOW 矩阵，词向量矩阵 $\rho$。
- Step 1: Encoder (推断)
  - 输入文档向量 $x_i$ ($1 \times 1024$)。
  - 经过 MLP (2层神经网络) $\to$ 输出均值 $\mu$ 和 对数方差 $\log\Sigma$。
- Step 2: Reparameterization (重参数化)
  - 采样噪声 $\epsilon \sim \mathcal{N}(0, 1)$。
  - 计算潜在变量 $z = \mu + \exp(0.5 \log\Sigma) \cdot \epsilon$。
  - 计算主题分布 $\theta = \text{Softmax}(z)$。
- Step 3: Decoder (生成)
  - 计算主题-词分布 $\beta = \text{Softmax}(\alpha \cdot \rho^T)$。
  - 预测文档词频 $\hat{x} = \theta \cdot \beta$。
- Step 4: Optimization (反向传播)
  - Loss = 重构误差 (NLL) + KL 散度 (正则项)。
  - 使用 AdamW 优化器更新 Encoder 参数和主题矩阵 $\alpha$。

---
六、 输出结果体系
6.1 Embedding 输出
- Dataset_Specific_Embeddings.npy ($N \times 1024$)：高精度的文档向量，可用于后续的聚类或 RAG 检索。
6.2 ETM 输出
- $$\theta$$ (Theta) 矩阵 ($N \times K$)：每篇文档的主题混合比例。
- $$\beta$$ (Beta) 矩阵 ($K \times V$)：每个主题下的词语分布（用于生成词云）。
- Topic Embeddings ($\alpha$) ($K \times 1024$)：主题本身的向量表示，可用于计算主题间的相关性。

---
七、 评估指标
- Topic Coherence (NPMI)：核心指标。衡量主题下的高频词在原始文本中是否经常一起出现。
- Topic Diversity (TD)：衡量不同主题之间的差异程度，避免主题趋同。
- Reconstruction Loss：监控 VAE 模型是否收敛。

---
八、 工程特性与稳定性设计
- 完全解耦 (Decoupling)：Embedding 生成与 Topic Modeling 是两个独立的 Python 进程，互不阻塞。
- 可复现性 (Reproducibility)：固定 Random Seed，确保每次运行结果一致。
- 断点续训：Qwen 微调和 ETM 训练均支持 Checkpoint 保存与加载。

---
九、 资源消耗评估 (RTX 4090)
- Qwen 微调：约 1-3 小时/数据集（取决于数据量）。
- Embedding 推理：约 10-20 分钟/数据集（Batch Size 可开大）。
- ETM 训练：约 5-10 分钟/数据集（因为输入是预计算好的向量，计算量极小）。
- 结论：该方案在单卡上运行效率极高，完全可行。