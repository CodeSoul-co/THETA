我的项目逻辑是：
1.数据清洗（已完成）
    （1）包含5个社会学文本数据集，详细数据介绍在/root/autodl-tmp/data/cleaned_data_report.md
    （2）其中两个数据集无标签，三个数据集有标签，采用两种不同的训练方法有监督，无监督，还有一种是zero-shot
2.qwen3-embedding-0.6B的模型训练（代替了原始的TF-DIF，用LLM的embedding向量化）
    （1）对于第一步已经处理好的数据（三个有label，两个无label）进行embedding训练，得到向量化的矩阵doc*word
    （2）确保这一步得到的向量化矩阵能够用于下一步的topicmodel的训练
    （3）先进行zero-shot训练
3.ETM模型训练
    （1）我打算先搭建好topicmodel的框架，这个先embedding训练一步完成，所以需要确保接口问题
    （2）你必须根据上一步的输出，创建好这一步的接口输入是没有问题的
    （3）确保接口没有问题之后，你需要参考/root/autodl-tmp/ETM/ETM_ref，这个是别人的代码，看看他用的是什么方法，得到什么中间参数矩阵，从而搭建我们自己的ETM框架
    （4）你需要保证训练得到的中间重要参数矩阵保留，需要在训练的代码中coding，这是为了可视化分析
    （5）留好下游任务的接口，因为我有5个数据集，每个数据集的下游任务有所不同，这个任务需要后续完成补充，你可以做一些简单的下游任务（可视化等等）

简要思路：
| 数据资产 | 训练范式 | 核心技术 | 核心产出 |
| 5 套数据集 (3 有标签, 2 无标签) | 自监督/无监督 | LoRA 领域适应性微调 | 5 个领域专家 LoRA 适配器 |
| 增强 Embedding | 无监督主题建模 | ETM  | 高纯度主题结构与可解释性 |
| 3 套标签数据 | 有监督验证 | 下游分类任务 (F1-Score) | 效度对比报告（证明 LoRA 优越性） |

## 💻 三步训练流程与数据流 (Data Workflow)
### 1. Zero-shot 预备 (特征提取与基线)
 目的： 利用通用 LLM 的预训练知识，提取所有文本的初始语义特征，并设定性能基线。
 动作： 使用 通用 LLM (如 qwen-3 Base) 不进行微调，提取 5 个数据集的原始 Embedding。
 产出： $M_{\text{Zero-shot}} \in \mathbb{R}^{N \times D}$ (初始 Embedding 矩阵)。
### 2. 领域适应性增强 (LoRA 核心)
目的：将领域专业知识注入到 Embedding 空间，克服 Zero-shot 的通用性限制。
 动作： 针对每个数据集，利用所有文本（无标签部分 + 标签文本）进行 自监督 LoRA 微调 (MLM/CLM)。
 产出： 5 个轻量级 LoRA 适配器。
### 3. ETM 主题建模与效度验证

| 子步骤 | 目标 | 数据利用 | 模型架构 |
| A. ETM 训练 (主题发现) | 发现高连贯性的潜在社会议题。 | 所有文本 | 将 LoRA 增强后的词向量 喂给 ETM 的 Decoder，同时训练 ETM 的 Encoder。 |
| B. 下游验证 | 验证 LoRA 增强特征在预测任务上的有效性。 | 3 个有标签数据集 | 使用 LoRA 增强 Embedding 进行有监督分类训练。 |
| 核心产出 | 主题分布矩阵、LoRA 适配器、$F1$-score 对比报告。 | 

## 🔬 ETM 架构与 Embedding 集成
ETM (Embedded Topic Model) 是一种基于 VAE 的主题模型，它通过集成词语 Embedding 来提高主题连贯性。 
 Encoder (推理网络): 在 ETM 训练时，用于学习文档到主题分布 $\theta_d$ 的映射。
 Decoder (生成网络): 负责利用 $\theta_d$ 和词语 Embedding 重构文档。
