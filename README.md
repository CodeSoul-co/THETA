# Qwen3-Embedding è®­ç»ƒæ“ä½œæŒ‡å—

## ğŸ“‹ ç›®æ ‡
ä½¿ç”¨ Qwen3-Embedding-0.6B æ¨¡å‹å¯¹5ä¸ªç¤¾ä¼šå­¦æ–‡æœ¬æ•°æ®é›†è¿›è¡Œå‘é‡åŒ–ï¼Œè¾“å‡º `docÃ—vector` çŸ©é˜µä¾›åç»­ ETM æ¨¡å‹è®­ç»ƒä½¿ç”¨ã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
/root/autodl-tmp/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cleaned_data_report.md          # æ•°æ®é›†è¯´æ˜æ–‡æ¡£
â”‚   â”œâ”€â”€ dataset1_labeled/               # æœ‰æ ‡ç­¾æ•°æ®é›†1
â”‚   â”œâ”€â”€ dataset2_labeled/               # æœ‰æ ‡ç­¾æ•°æ®é›†2
â”‚   â”œâ”€â”€ dataset3_labeled/               # æœ‰æ ‡ç­¾æ•°æ®é›†3
â”‚   â”œâ”€â”€ dataset4_unlabeled/             # æ— æ ‡ç­¾æ•°æ®é›†4
â”‚   â””â”€â”€ dataset5_unlabeled/             # æ— æ ‡ç­¾æ•°æ®é›†5
â”œâ”€â”€ embedding/
â”‚   â”œâ”€â”€ configs/                        # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ scripts/                        # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ outputs/                        # è¾“å‡ºç›®å½•
â”‚   â”‚   â”œâ”€â”€ zero_shot/                  # Zero-shotç»“æœ
â”‚   â”‚   â”œâ”€â”€ supervised/                 # æœ‰ç›‘ç£è®­ç»ƒç»“æœ
â”‚   â”‚   â””â”€â”€ unsupervised/               # æ— ç›‘ç£è®­ç»ƒç»“æœ
â”‚   â”œâ”€â”€ checkpoints/                    # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â””â”€â”€ logs/                           # è®­ç»ƒæ—¥å¿—
â””â”€â”€ ETM/                                # ä¸‹æ¸¸ETMæ¨¡å‹ç›®å½•
```

## ğŸš€ ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Embeddingç›¸å…³
pip install transformers>=4.51.0
pip install sentence-transformers>=2.7.0
pip install ms-swift
pip install peft
pip install datasets

# å·¥å…·åº“
pip install numpy pandas scikit-learn
pip install matplotlib seaborn
pip install tqdm
pip install tensorboard
```

### 2. éªŒè¯ç¯å¢ƒ

```bash
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "from transformers import AutoModel; print('Transformers OK')"
```

## ğŸ“Š æ•°æ®å‡†å¤‡

### 1. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§

```bash
cd /root/autodl-tmp/data
cat cleaned_data_report.md
```

### 2. åˆ›å»ºæ•°æ®åŠ è½½è„šæœ¬

åœ¨ `/root/autodl-tmp/embedding/scripts/data_loader.py` åˆ›å»ºï¼š

```python
import os
import json
import pandas as pd
from typing import List, Dict, Tuple

class DatasetLoader:
    """ç»Ÿä¸€çš„æ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, base_path: str = "/root/autodl-tmp/data"):
        self.base_path = base_path
        
    def load_dataset(self, dataset_name: str) -> Tuple[List[str], List]:
        """
        åŠ è½½å•ä¸ªæ•°æ®é›†
        
        Returns:
            texts: æ–‡æœ¬åˆ—è¡¨
            labels: æ ‡ç­¾åˆ—è¡¨ï¼ˆå¦‚æœæ— æ ‡ç­¾åˆ™ä¸ºNoneï¼‰
        """
        dataset_path = os.path.join(self.base_path, dataset_name)
        
        # TODO: æ ¹æ®å®é™…æ•°æ®æ ¼å¼è°ƒæ•´
        # å‡è®¾æ•°æ®æ ¼å¼ä¸ºCSVæˆ–JSON
        if os.path.exists(os.path.join(dataset_path, "data.csv")):
            df = pd.read_csv(os.path.join(dataset_path, "data.csv"))
            texts = df['text'].tolist()
            labels = df['label'].tolist() if 'label' in df.columns else None
        elif os.path.exists(os.path.join(dataset_path, "data.json")):
            with open(os.path.join(dataset_path, "data.json"), 'r', encoding='utf-8') as f:
                data = json.load(f)
            texts = [item['text'] for item in data]
            labels = [item.get('label') for item in data]
            if all(l is None for l in labels):
                labels = None
        else:
            raise FileNotFoundError(f"No data file found in {dataset_path}")
            
        return texts, labels
    
    def get_all_datasets(self) -> Dict[str, Tuple[List[str], List]]:
        """åŠ è½½æ‰€æœ‰æ•°æ®é›†"""
        datasets = {}
        dataset_names = [
            "dataset1_labeled",
            "dataset2_labeled", 
            "dataset3_labeled",
            "dataset4_unlabeled",
            "dataset5_unlabeled"
        ]
        
        for name in dataset_names:
            texts, labels = self.load_dataset(name)
            datasets[name] = (texts, labels)
            print(f"Loaded {name}: {len(texts)} samples, labeled: {labels is not None}")
            
        return datasets
```

## ğŸ¯ æ–¹æ³•ä¸€ï¼šZero-Shot Embedding

### æ¦‚è¿°
ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒçš„ Qwen3-Embedding-0.6B æ¨¡å‹ï¼Œæ— éœ€è®­ç»ƒï¼Œå¿«é€Ÿè·å–åŸºçº¿å‘é‡è¡¨ç¤ºã€‚

### ä½¿ç”¨åœºæ™¯
- æ‰€æœ‰5ä¸ªæ•°æ®é›†
- ä½œä¸ºæ€§èƒ½åŸºçº¿
- å¿«é€ŸåŸå‹éªŒè¯

### å®ç°æ­¥éª¤

#### 1. åˆ›å»º Zero-Shot è„šæœ¬

åœ¨ `/root/autodl-tmp/embedding/scripts/zero_shot_embedding.py` åˆ›å»ºï¼š

```python
import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import os
import pickle
from data_loader import DatasetLoader

class ZeroShotEmbedder:
    """Zero-shot embeddingç”Ÿæˆå™¨"""
    
    def __init__(self, model_name: str = "Qwen/Qwen3-Embedding-0.6B"):
        print(f"Loading model: {model_name}")
        self.model = SentenceTransformer(model_name)
        self.model.eval()
        
        # å¦‚æœæœ‰GPUï¼Œä½¿ç”¨GPU
        if torch.cuda.is_available():
            self.model = self.model.cuda()
            print(f"Using GPU: {torch.cuda.get_device_name(0)}")
        else:
            print("Using CPU")
    
    def embed_texts(self, texts: list, batch_size: int = 32) -> np.ndarray:
        """
        æ‰¹é‡ç”Ÿæˆembeddings
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            embeddings: (num_docs, embedding_dim) çš„numpyæ•°ç»„
        """
        embeddings = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="Embedding"):
            batch = texts[i:i + batch_size]
            with torch.no_grad():
                batch_emb = self.model.encode(
                    batch,
                    convert_to_numpy=True,
                    show_progress_bar=False,
                    normalize_embeddings=True  # L2å½’ä¸€åŒ–
                )
            embeddings.append(batch_emb)
        
        return np.vstack(embeddings)
    
    def save_embeddings(self, embeddings: np.ndarray, output_path: str):
        """ä¿å­˜embeddings"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ä¿å­˜ä¸º.npyæ ¼å¼ï¼ˆæ¨èç”¨äºETMï¼‰
        np.save(output_path + '.npy', embeddings)
        
        # ä¹Ÿä¿å­˜ä¸º.pklæ ¼å¼ï¼ˆå¤‡ç”¨ï¼‰
        with open(output_path + '.pkl', 'wb') as f:
            pickle.dump(embeddings, f)
        
        print(f"Saved embeddings to {output_path}")
        print(f"Shape: {embeddings.shape}")

def main():
    # é…ç½®
    output_dir = "/root/autodl-tmp/embedding/outputs/zero_shot"
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    loader = DatasetLoader()
    datasets = loader.get_all_datasets()
    
    # åˆå§‹åŒ–embedder
    embedder = ZeroShotEmbedder()
    
    # å¯¹æ¯ä¸ªæ•°æ®é›†ç”Ÿæˆembeddings
    for dataset_name, (texts, labels) in datasets.items():
        print(f"\n{'='*50}")
        print(f"Processing {dataset_name}")
        print(f"{'='*50}")
        
        # ç”Ÿæˆembeddings
        embeddings = embedder.embed_texts(texts, batch_size=32)
        
        # ä¿å­˜
        output_path = os.path.join(output_dir, f"{dataset_name}_embeddings")
        embedder.save_embeddings(embeddings, output_path)
        
        # å¦‚æœæœ‰æ ‡ç­¾ï¼Œä¹Ÿä¿å­˜æ ‡ç­¾
        if labels is not None:
            label_path = os.path.join(output_dir, f"{dataset_name}_labels.npy")
            np.save(label_path, np.array(labels))
            print(f"Saved labels to {label_path}")
    
    print("\nâœ… Zero-shot embedding completed!")

if __name__ == "__main__":
    main()
```

#### 2. è¿è¡Œ Zero-Shot

```bash
cd /root/autodl-tmp/embedding/scripts
python zero_shot_embedding.py
```

#### 3. éªŒè¯è¾“å‡º

```python
# æ£€æŸ¥è¾“å‡º
import numpy as np

# åŠ è½½embeddings
embeddings = np.load('/root/autodl-tmp/embedding/outputs/zero_shot/dataset1_labeled_embeddings.npy')
print(f"Embeddings shape: {embeddings.shape}")  # åº”è¯¥æ˜¯ (num_docs, 768)
print(f"Embedding stats: mean={embeddings.mean():.4f}, std={embeddings.std():.4f}")
```

---

## ğŸ“ æ–¹æ³•äºŒï¼šæœ‰ç›‘ç£å­¦ä¹ ï¼ˆLoRAå¾®è°ƒï¼‰

### æ¦‚è¿°
ä½¿ç”¨æœ‰æ ‡ç­¾æ•°æ®ï¼ˆ3ä¸ªæ•°æ®é›†ï¼‰è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œé€šè¿‡ LoRA æ–¹æ³•é«˜æ•ˆè°ƒæ•´æ¨¡å‹å‚æ•°ã€‚

### ä½¿ç”¨åœºæ™¯
- dataset1_labeled
- dataset2_labeled
- dataset3_labeled

### å…³é”®æŠ€æœ¯
- **LoRA (Low-Rank Adaptation)**: ä½ç§©åˆ†è§£ï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°
- **å¯¹æ¯”å­¦ä¹ æŸå¤±**: æ­£æ ·æœ¬æ‹‰è¿‘ï¼Œè´Ÿæ ·æœ¬æ¨è¿œ
- **äº¤å‰ç†µæŸå¤±**: ä¼˜åŒ–åˆ†ç±»è¾¹ç•Œ

### å®ç°æ­¥éª¤

#### 1. å‡†å¤‡è®­ç»ƒæ•°æ®æ ¼å¼

åœ¨ `/root/autodl-tmp/embedding/scripts/prepare_supervised_data.py` åˆ›å»ºï¼š

```python
import json
import random
from typing import List, Tuple
from data_loader import DatasetLoader

class SupervisedDataPreparer:
    """å‡†å¤‡æœ‰ç›‘ç£è®­ç»ƒæ•°æ®"""
    
    def __init__(self, negative_ratio: int = 5):
        """
        Args:
            negative_ratio: è´Ÿæ ·æœ¬æ•°é‡ç›¸å¯¹äºæ­£æ ·æœ¬çš„æ¯”ä¾‹
        """
        self.negative_ratio = negative_ratio
    
    def create_triplets(self, texts: List[str], labels: List) -> List[dict]:
        """
        åˆ›å»ºä¸‰å…ƒç»„æ•°æ®: (query, positive, negative)
        
        ç­–ç•¥ï¼š
        - åŒä¸€æ ‡ç­¾çš„æ ·æœ¬äº’ä¸ºæ­£æ ·æœ¬
        - ä¸åŒæ ‡ç­¾çš„æ ·æœ¬ä¸ºè´Ÿæ ·æœ¬
        """
        # æŒ‰æ ‡ç­¾åˆ†ç»„
        label_to_texts = {}
        for text, label in zip(texts, labels):
            if label not in label_to_texts:
                label_to_texts[label] = []
            label_to_texts[label].append(text)
        
        triplets = []
        all_labels = list(label_to_texts.keys())
        
        for label, label_texts in label_to_texts.items():
            # å¦‚æœè¯¥æ ‡ç­¾ä¸‹æ ·æœ¬å¤ªå°‘ï¼Œè·³è¿‡
            if len(label_texts) < 2:
                continue
            
            # ä¸ºæ¯ä¸ªæ–‡æœ¬åˆ›å»ºè®­ç»ƒæ ·æœ¬
            for i, query in enumerate(label_texts):
                # æ­£æ ·æœ¬ï¼šåŒæ ‡ç­¾çš„å…¶ä»–æ–‡æœ¬
                positive_candidates = [t for j, t in enumerate(label_texts) if j != i]
                if not positive_candidates:
                    continue
                
                # è´Ÿæ ·æœ¬ï¼šä¸åŒæ ‡ç­¾çš„æ–‡æœ¬
                negative_labels = [l for l in all_labels if l != label]
                negatives = []
                for _ in range(self.negative_ratio):
                    neg_label = random.choice(negative_labels)
                    neg_text = random.choice(label_to_texts[neg_label])
                    negatives.append(neg_text)
                
                triplets.append({
                    "query": query,
                    "positive": random.choice(positive_candidates),
                    "negatives": negatives,
                    "label": label
                })
        
        return triplets
    
    def save_to_jsonl(self, triplets: List[dict], output_path: str):
        """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
        with open(output_path, 'w', encoding='utf-8') as f:
            for triplet in triplets:
                f.write(json.dumps(triplet, ensure_ascii=False) + '\n')
        print(f"Saved {len(triplets)} triplets to {output_path}")

def main():
    loader = DatasetLoader()
    preparer = SupervisedDataPreparer(negative_ratio=5)
    
    labeled_datasets = [
        "dataset1_labeled",
        "dataset2_labeled",
        "dataset3_labeled"
    ]
    
    for dataset_name in labeled_datasets:
        print(f"\nProcessing {dataset_name}...")
        texts, labels = loader.load_dataset(dataset_name)
        
        # åˆ›å»ºä¸‰å…ƒç»„
        triplets = preparer.create_triplets(texts, labels)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        random.shuffle(triplets)
        split_idx = int(len(triplets) * 0.9)
        train_triplets = triplets[:split_idx]
        val_triplets = triplets[split_idx:]
        
        # ä¿å­˜
        output_dir = f"/root/autodl-tmp/embedding/outputs/supervised/{dataset_name}"
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        preparer.save_to_jsonl(train_triplets, f"{output_dir}/train.jsonl")
        preparer.save_to_jsonl(val_triplets, f"{output_dir}/val.jsonl")
        
        print(f"Train: {len(train_triplets)}, Val: {len(val_triplets)}")

if __name__ == "__main__":
    main()
```

#### 2. åˆ›å»º LoRA è®­ç»ƒè„šæœ¬

åœ¨ `/root/autodl-tmp/embedding/scripts/train_supervised_lora.py` åˆ›å»ºï¼š

```python
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from peft import LoraConfig, get_peft_model, TaskType
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import json
import os
import numpy as np

class TripletDataset(Dataset):
    """ä¸‰å…ƒç»„æ•°æ®é›†"""
    
    def __init__(self, jsonl_path: str, tokenizer, max_length: int = 512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []
        
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                self.data.append(json.loads(line))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # Tokenize
        query = self.tokenizer(
            item['query'],
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        positive = self.tokenizer(
            item['positive'],
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # éšæœºé€‰ä¸€ä¸ªè´Ÿæ ·æœ¬
        import random
        negative_text = random.choice(item['negatives'])
        negative = self.tokenizer(
            negative_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        return {
            'query_input_ids': query['input_ids'].squeeze(0),
            'query_attention_mask': query['attention_mask'].squeeze(0),
            'positive_input_ids': positive['input_ids'].squeeze(0),
            'positive_attention_mask': positive['attention_mask'].squeeze(0),
            'negative_input_ids': negative['input_ids'].squeeze(0),
            'negative_attention_mask': negative['attention_mask'].squeeze(0),
        }

class TripletLoss(nn.Module):
    """ä¸‰å…ƒç»„æŸå¤±"""
    
    def __init__(self, margin: float = 0.5):
        super().__init__()
        self.margin = margin
    
    def forward(self, anchor, positive, negative):
        """
        è®¡ç®—ä¸‰å…ƒç»„æŸå¤±
        
        Loss = max(0, d(anchor, positive) - d(anchor, negative) + margin)
        """
        pos_dist = torch.sum((anchor - positive) ** 2, dim=1)
        neg_dist = torch.sum((anchor - negative) ** 2, dim=1)
        
        loss = torch.relu(pos_dist - neg_dist + self.margin)
        return loss.mean()

def mean_pooling(token_embeddings, attention_mask):
    """å‡å€¼æ± åŒ–"""
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

class LoRATrainer:
    """LoRAè®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen3-Embedding-0.6B",
        lora_r: int = 8,
        lora_alpha: int = 32,
        lora_dropout: float = 0.1
    ):
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        base_model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
        
        # é…ç½®LoRA
        lora_config = LoraConfig(
            task_type=TaskType.FEATURE_EXTRACTION,
            r=lora_r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # å¯¹æ³¨æ„åŠ›å±‚åº”ç”¨LoRA
            bias="none"
        )
        
        # åº”ç”¨LoRA
        self.model = get_peft_model(base_model, lora_config)
        self.model.print_trainable_parameters()
        
        # æŸå¤±å‡½æ•°
        self.criterion = TripletLoss(margin=0.5)
        
        # ç§»åŠ¨åˆ°GPU
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.model.to(self.device)
    
    def encode(self, input_ids, attention_mask):
        """ç¼–ç æ–‡æœ¬"""
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
        embeddings = mean_pooling(outputs.last_hidden_state, attention_mask)
        # L2å½’ä¸€åŒ–
        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
        return embeddings
    
    def train_epoch(self, dataloader, optimizer):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        
        progress_bar = tqdm(dataloader, desc="Training")
        for batch in progress_bar:
            # ç§»åŠ¨åˆ°GPU
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # ç¼–ç 
            query_emb = self.encode(batch['query_input_ids'], batch['query_attention_mask'])
            pos_emb = self.encode(batch['positive_input_ids'], batch['positive_attention_mask'])
            neg_emb = self.encode(batch['negative_input_ids'], batch['negative_attention_mask'])
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(query_emb, pos_emb, neg_emb)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            progress_bar.set_postfix({'loss': loss.item()})
        
        return total_loss / len(dataloader)
    
    @torch.no_grad()
    def evaluate(self, dataloader):
        """è¯„ä¼°"""
        self.model.eval()
        total_loss = 0
        
        for batch in tqdm(dataloader, desc="Evaluating"):
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            query_emb = self.encode(batch['query_input_ids'], batch['query_attention_mask'])
            pos_emb = self.encode(batch['positive_input_ids'], batch['positive_attention_mask'])
            neg_emb = self.encode(batch['negative_input_ids'], batch['negative_attention_mask'])
            
            loss = self.criterion(query_emb, pos_emb, neg_emb)
            total_loss += loss.item()
        
        return total_loss / len(dataloader)
    
    def save_model(self, output_dir: str):
        """ä¿å­˜LoRAæƒé‡"""
        os.makedirs(output_dir, exist_ok=True)
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        print(f"Model saved to {output_dir}")

def train_dataset(dataset_name: str, num_epochs: int = 3, batch_size: int = 16, learning_rate: float = 2e-4):
    """è®­ç»ƒå•ä¸ªæ•°æ®é›†"""
    print(f"\n{'='*60}")
    print(f"Training on {dataset_name}")
    print(f"{'='*60}")
    
    # è·¯å¾„
    data_dir = f"/root/autodl-tmp/embedding/outputs/supervised/{dataset_name}"
    output_dir = f"/root/autodl-tmp/embedding/checkpoints/supervised/{dataset_name}"
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = LoRATrainer(lora_r=8, lora_alpha=32)
    
    # å‡†å¤‡æ•°æ®
    train_dataset = TripletDataset(f"{data_dir}/train.jsonl", trainer.tokenizer)
    val_dataset = TripletDataset(f"{data_dir}/val.jsonl", trainer.tokenizer)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(trainer.model.parameters(), lr=learning_rate)
    
    # è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        
        train_loss = trainer.train_epoch(train_loader, optimizer)
        val_loss = trainer.evaluate(val_loader)
        
        print(f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            trainer.save_model(output_dir)
            print(f"âœ“ New best model saved!")
    
    return output_dir

def main():
    datasets = ["dataset1_labeled", "dataset2_labeled", "dataset3_labeled"]
    
    for dataset_name in datasets:
        model_path = train_dataset(
            dataset_name,
            num_epochs=5,
            batch_size=16,
            learning_rate=2e-4
        )
        print(f"\nâœ… Completed training for {dataset_name}")
        print(f"Model saved at: {model_path}")

if __name__ == "__main__":
    main()
```

#### 3. è¿è¡Œç›‘ç£è®­ç»ƒ

```bash
# 1. å‡†å¤‡è®­ç»ƒæ•°æ®
cd /root/autodl-tmp/embedding/scripts
python prepare_supervised_data.py

# 2. å¼€å§‹è®­ç»ƒ
python train_supervised_lora.py
```

#### 4. ç”Ÿæˆå¾®è°ƒåçš„embeddings

åœ¨ `/root/autodl-tmp/embedding/scripts/generate_supervised_embeddings.py` åˆ›å»ºï¼š

```python
import torch
from transformers import AutoTokenizer, AutoModel
from peft import PeftModel
import numpy as np
from tqdm import tqdm
import os
from data_loader import DatasetLoader

def mean_pooling(token_embeddings, attention_mask):
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

def generate_embeddings(dataset_name: str, model_path: str, batch_size: int = 32):
    """ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆembeddings"""
    
    # åŠ è½½æ¨¡å‹
    print(f"Loading model from {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    base_model = AutoModel.from_pretrained("Qwen/Qwen3-Embedding-0.6B", trust_remote_code=True)
    model = PeftModel.from_pretrained(base_model, model_path)
    model.eval()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    # åŠ è½½æ•°æ®
    loader = DatasetLoader()
    texts, labels = loader.load_dataset(dataset_name)
    
    # ç”Ÿæˆembeddings
    all_embeddings = []
    with torch.no_grad():
        for i in tqdm(range(0, len(texts), batch_size), desc="Generating embeddings"):
            batch_texts = texts[i:i + batch_size]
            
            # Tokenize
            encoded = tokenizer(
                batch_texts,
                max_length=512,
                padding=True,
                truncation=True,
                return_tensors='pt'
            ).to(device)
            
            # Forward
            outputs = model(**encoded)
            embeddings = mean_pooling(outputs.last_hidden_state, encoded['attention_mask'])
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
            
            all_embeddings.append(embeddings.cpu().numpy())
    
    embeddings_matrix = np.vstack(all_embeddings)
    
    # ä¿å­˜
    output_dir = "/root/autodl-tmp/embedding/outputs/supervised"
    os.makedirs(output_dir, exist_ok=True)
    
    output_path = os.path.join(output_dir, f"{dataset_name}_lora_embeddings")
    np.save(output_path + '.npy', embeddings_matrix)
    
    if labels is not None:
        np.save(os.path.join(output_dir, f"{dataset_name}_labels.npy"), np.array(labels))
    
    print(f"Saved embeddings: {embeddings_matrix.shape}")
    return embeddings_matrix

def main():
    datasets = ["dataset1_labeled", "dataset2_labeled", "dataset3_labeled"]
    
    for dataset_name in datasets:
        model_path = f"/root/autodl-tmp/embedding/checkpoints/supervised/{dataset_name}"
        generate_embeddings(dataset_name, model_path)
        print(f"âœ… Generated embeddings for {dataset_name}\n")

if __name__ == "__main__":
    main()
```

```bash
# ç”Ÿæˆå¾®è°ƒåçš„embeddings
python generate_supervised_embeddings.py
```

---

## ğŸ”„ æ–¹æ³•ä¸‰ï¼šæ— ç›‘ç£å­¦ä¹ ï¼ˆè‡ªå›å½’ + KLæ•£åº¦ï¼‰

### æ¦‚è¿°
å¯¹æ— æ ‡ç­¾æ•°æ®ï¼ˆ2ä¸ªæ•°æ®é›†ï¼‰ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ ï¼Œé€šè¿‡æ©ç è¯­è¨€æ¨¡å‹(MLM)æˆ–è‡ªå›å½’é¢„æµ‹ä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚

### ä½¿ç”¨åœºæ™¯
- dataset4_unlabeled
- dataset5_unlabeled

### å…³é”®æŠ€æœ¯
- **Masked Language Modeling (MLM)**: æ©ç é¢„æµ‹
- **Autoregressive Prediction**: è‡ªå›å½’é¢„æµ‹
- **KL Divergence**: è¡¡é‡åˆ†å¸ƒå·®å¼‚

### å®ç°æ­¥éª¤

#### 1. åˆ›å»ºæ— ç›‘ç£è®­ç»ƒè„šæœ¬

åœ¨ `/root/autodl-tmp/embedding/scripts/train_unsupervised.py` åˆ›å»ºï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
from peft import LoraConfig, get_peft_model, TaskType
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import random
import os
import numpy as np
from data_loader import DatasetLoader

class MLMDataset(Dataset):
    """æ©ç è¯­è¨€æ¨¡å‹æ•°æ®é›†"""
    
    def __init__(self, texts: list, tokenizer, max_length: int = 512, mlm_probability: float = 0.15):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.mlm_probability = mlm_probability
    
    def __len__(self):
        return len(self.texts)
    
    def mask_tokens(self, inputs):
        """éšæœºæ©ç token"""
        labels = inputs.clone()
        
        # åˆ›å»ºæ©ç çŸ©é˜µ
        probability_matrix = torch.full(labels.shape, self.mlm_probability)
        
        # ä¸æ©ç ç‰¹æ®Štoken
        special_tokens_mask = [
            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True)
            for val in labels.tolist()
        ]
        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)
        
        masked_indices = torch.bernoulli(probability_matrix).bool()
        labels[~masked_indices] = -100  # åªè®¡ç®—è¢«æ©ç ä½ç½®çš„æŸå¤±
        
        # 80%æ›¿æ¢ä¸º[MASK], 10%éšæœºæ›¿æ¢, 10%ä¿æŒä¸å˜
        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices
        inputs[indices_replaced] = self.tokenizer.mask_token_id
        
        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced
        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)
        inputs[indices_random] = random_words[indices_random]
        
        return inputs, labels
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        
        # Tokenize
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].squeeze(0)
        attention_mask = encoding['attention_mask'].squeeze(0)
        
        # åº”ç”¨æ©ç 
        masked_input_ids, labels = self.mask_tokens(input_ids)
        
        return {
            'input_ids': masked_input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }

class KLDivergenceLoss(nn.Module):
    """KLæ•£åº¦æŸå¤±"""
    
    def __init__(self, temperature: float = 1.0):
        super().__init__()
        self.temperature = temperature
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
    
    def forward(self, student_logits, teacher_logits):
        """
        è®¡ç®—å­¦ç”Ÿæ¨¡å‹å’Œæ•™å¸ˆæ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„KLæ•£åº¦
        
        Args:
            student_logits: å­¦ç”Ÿæ¨¡å‹çš„logits
            teacher_logits: æ•™å¸ˆæ¨¡å‹çš„logits (å¯ä»¥æ˜¯åŸå§‹æ¨¡å‹æˆ–ç›®æ ‡åˆ†å¸ƒ)
        """
        # åº”ç”¨æ¸©åº¦ç¼©æ”¾ï¼Œä½¿åˆ†å¸ƒæ›´å¹³æ»‘
        student_probs = F.log_softmax(student_logits / self.temperature, dim=-1)
        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)
        
        # è®¡ç®—KLæ•£åº¦
        kl_div = self.kl_loss(student_probs, teacher_probs) * (self.temperature ** 2)
        
        return kl_div

class UnsupervisedTrainer:
    """æ— ç›‘ç£è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen3-Embedding-0.6B",
        lora_r: int = 8,
        lora_alpha: int = 16,
        use_kl_loss: bool = True
    ):
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        
        # åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆä½œä¸ºæ•™å¸ˆï¼‰
        if use_kl_loss:
            self.teacher_model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
            self.teacher_model.eval()
            for param in self.teacher_model.parameters():
                param.requires_grad = False
        else:
            self.teacher_model = None
        
        # åŠ è½½å­¦ç”Ÿæ¨¡å‹å¹¶åº”ç”¨LoRA
        student_model = AutoModel.from_pretrained(model_name, trust_remote_code=True)
        
        lora_config = LoraConfig(
            task_type=TaskType.FEATURE_EXTRACTION,
            r=lora_r,
            lora_alpha=lora_alpha,
            lora_dropout=0.1,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            bias="none"
        )
        
        self.model = get_peft_model(student_model, lora_config)
        self.model.print_trainable_parameters()
        
        # æŸå¤±å‡½æ•°
        self.mlm_loss = nn.CrossEntropyLoss(ignore_index=-100)
        self.kl_loss = KLDivergenceLoss(temperature=2.0) if use_kl_loss else None
        
        # è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.model.to(self.device)
        if self.teacher_model:
            self.teacher_model = self.teacher_model.to(self.device)
    
    def train_epoch(self, dataloader, optimizer, alpha_mlm: float = 0.7, alpha_kl: float = 0.3):
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            alpha_mlm: MLMæŸå¤±æƒé‡
            alpha_kl: KLæ•£åº¦æŸå¤±æƒé‡
        """
        self.model.train()
        total_loss = 0
        total_mlm_loss = 0
        total_kl_loss = 0
        
        progress_bar = tqdm(dataloader, desc="Training")
        for batch in progress_bar:
            batch = {k: v.to(self.device) for k, v in batch.items()}
            
            # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
            student_outputs = self.model(
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask']
            )
            
            # MLMæŸå¤±
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ·»åŠ è¯­è¨€æ¨¡å‹å¤´ï¼Œç®€åŒ–ç‰ˆæœ¬ä½¿ç”¨last_hidden_state
            # å®é™…åº”ç”¨ä¸­éœ€è¦æ·»åŠ lm_headå±‚
            mlm_loss = torch.tensor(0.0).to(self.device)  # å ä½ç¬¦
            
            # KLæ•£åº¦æŸå¤±
            kl_loss = torch.tensor(0.0).to(self.device)
            if self.teacher_model and self.kl_loss:
                with torch.no_grad():
                    teacher_outputs = self.teacher_model(
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask']
                    )
                
                # è®¡ç®—éšè—çŠ¶æ€çš„KLæ•£åº¦
                # ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å‡å€¼æ± åŒ–åçš„å‘é‡
                student_hidden = student_outputs.last_hidden_state.mean(dim=1)
                teacher_hidden = teacher_outputs.last_hidden_state.mean(dim=1)
                
                # å°†å‘é‡è½¬æ¢ä¸ºåˆ†å¸ƒï¼ˆç®€åŒ–ï¼‰
                kl_loss = F.mse_loss(student_hidden, teacher_hidden)  # ç®€åŒ–ç‰ˆKL
            
            # ç»„åˆæŸå¤±
            loss = alpha_mlm * mlm_loss + alpha_kl * kl_loss
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            total_mlm_loss += mlm_loss.item()
            total_kl_loss += kl_loss.item()
            
            progress_bar.set_postfix({
                'loss': loss.item(),
                'mlm': mlm_loss.item(),
                'kl': kl_loss.item()
            })
        
        avg_loss = total_loss / len(dataloader)
        avg_mlm = total_mlm_loss / len(dataloader)
        avg_kl = total_kl_loss / len(dataloader)
        
        return avg_loss, avg_mlm, avg_kl
    
    def save_model(self, output_dir: str):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(output_dir, exist_ok=True)
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        print(f"Model saved to {output_dir}")

def train_unsupervised_dataset(dataset_name: str, num_epochs: int = 3, batch_size: int = 16):
    """è®­ç»ƒå•ä¸ªæ— æ ‡ç­¾æ•°æ®é›†"""
    print(f"\n{'='*60}")
    print(f"Unsupervised training on {dataset_name}")
    print(f"{'='*60}")
    
    # åŠ è½½æ•°æ®
    loader = DatasetLoader()
    texts, _ = loader.load_dataset(dataset_name)
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = UnsupervisedTrainer(lora_r=8, lora_alpha=16, use_kl_loss=True)
    
    # å‡†å¤‡æ•°æ®
    train_size = int(0.9 * len(texts))
    train_texts = texts[:train_size]
    val_texts = texts[train_size:]
    
    train_dataset = MLMDataset(train_texts, trainer.tokenizer)
    val_dataset = MLMDataset(val_texts, trainer.tokenizer)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(trainer.model.parameters(), lr=2e-4)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        
        loss, mlm_loss, kl_loss = trainer.train_epoch(
            train_loader, 
            optimizer,
            alpha_mlm=0.7,
            alpha_kl=0.3
        )
        
        print(f"Loss: {loss:.4f}, MLM: {mlm_loss:.4f}, KL: {kl_loss:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    output_dir = f"/root/autodl-tmp/embedding/checkpoints/unsupervised/{dataset_name}"
    trainer.save_model(output_dir)
    
    return output_dir

def main():
    unlabeled_datasets = ["dataset4_unlabeled", "dataset5_unlabeled"]
    
    for dataset_name in unlabeled_datasets:
        model_path = train_unsupervised_dataset(
            dataset_name,
            num_epochs=5,
            batch_size=16
        )
        print(f"\nâœ… Completed unsupervised training for {dataset_name}")
        print(f"Model saved at: {model_path}")

if __name__ == "__main__":
    main()
```

#### 2. è¿è¡Œæ— ç›‘ç£è®­ç»ƒ

```bash
cd /root/autodl-tmp/embedding/scripts
python train_unsupervised.py
```

#### 3. ç”Ÿæˆæ— ç›‘ç£embeddings

åœ¨ `/root/autodl-tmp/embedding/scripts/generate_unsupervised_embeddings.py` åˆ›å»ºï¼š

```python
import torch
from transformers import AutoTokenizer, AutoModel
from peft import PeftModel
import numpy as np
from tqdm import tqdm
import os
from data_loader import DatasetLoader

def mean_pooling(token_embeddings, attention_mask):
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

def generate_unsupervised_embeddings(dataset_name: str, batch_size: int = 32):
    """ä½¿ç”¨æ— ç›‘ç£è®­ç»ƒçš„æ¨¡å‹ç”Ÿæˆembeddings"""
    
    # æ¨¡å‹è·¯å¾„
    model_path = f"/root/autodl-tmp/embedding/checkpoints/unsupervised/{dataset_name}"
    
    print(f"Loading model from {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    base_model = AutoModel.from_pretrained("Qwen/Qwen3-Embedding-0.6B", trust_remote_code=True)
    model = PeftModel.from_pretrained(base_model, model_path)
    model.eval()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    # åŠ è½½æ•°æ®
    loader = DatasetLoader()
    texts, _ = loader.load_dataset(dataset_name)
    
    # ç”Ÿæˆembeddings
    all_embeddings = []
    with torch.no_grad():
        for i in tqdm(range(0, len(texts), batch_size), desc="Generating embeddings"):
            batch_texts = texts[i:i + batch_size]
            
            encoded = tokenizer(
                batch_texts,
                max_length=512,
                padding=True,
                truncation=True,
                return_tensors='pt'
            ).to(device)
            
            outputs = model(**encoded)
            embeddings = mean_pooling(outputs.last_hidden_state, encoded['attention_mask'])
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
            
            all_embeddings.append(embeddings.cpu().numpy())
    
    embeddings_matrix = np.vstack(all_embeddings)
    
    # ä¿å­˜
    output_dir = "/root/autodl-tmp/embedding/outputs/unsupervised"
    os.makedirs(output_dir, exist_ok=True)
    
    output_path = os.path.join(output_dir, f"{dataset_name}_unsupervised_embeddings")
    np.save(output_path + '.npy', embeddings_matrix)
    
    print(f"Saved embeddings: {embeddings_matrix.shape}")
    return embeddings_matrix

def main():
    unlabeled_datasets = ["dataset4_unlabeled", "dataset5_unlabeled"]
    
    for dataset_name in unlabeled_datasets:
        generate_unsupervised_embeddings(dataset_name)
        print(f"âœ… Generated embeddings for {dataset_name}\n")

if __name__ == "__main__":
    main()
```

```bash
python generate_unsupervised_embeddings.py
```

---

## ğŸ“¦ è¾“å‡ºæ ¼å¼è¯´æ˜

### 1. æ–‡ä»¶ç»“æ„

```
/root/autodl-tmp/embedding/outputs/
â”œâ”€â”€ zero_shot/
â”‚   â”œâ”€â”€ dataset1_labeled_embeddings.npy           # (N, 768)
â”‚   â”œâ”€â”€ dataset1_labeled_labels.npy               # (N,)
â”‚   â”œâ”€â”€ dataset2_labeled_embeddings.npy
â”‚   â”œâ”€â”€ dataset2_labeled_labels.npy
â”‚   â”œâ”€â”€ dataset3_labeled_embeddings.npy
â”‚   â”œâ”€â”€ dataset3_labeled_labels.npy
â”‚   â”œâ”€â”€ dataset4_unlabeled_embeddings.npy
â”‚   â””â”€â”€ dataset5_unlabeled_embeddings.npy
â”œâ”€â”€ supervised/
â”‚   â”œâ”€â”€ dataset1_labeled_lora_embeddings.npy
â”‚   â”œâ”€â”€ dataset1_labeled_labels.npy
â”‚   â”œâ”€â”€ dataset2_labeled_lora_embeddings.npy
â”‚   â”œâ”€â”€ dataset2_labeled_labels.npy
â”‚   â”œâ”€â”€ dataset3_labeled_lora_embeddings.npy
â”‚   â””â”€â”€ dataset3_labeled_labels.npy
â””â”€â”€ unsupervised/
    â”œâ”€â”€ dataset4_unlabeled_unsupervised_embeddings.npy
    â””â”€â”€ dataset5_unlabeled_unsupervised_embeddings.npy
```

### 2. Numpyæ•°ç»„æ ¼å¼

æ‰€æœ‰embeddingæ–‡ä»¶ä¸º `.npy` æ ¼å¼:
- **Shape**: `(num_documents, embedding_dim)`
- **Dtype**: `float32`
- **Normalized**: L2å½’ä¸€åŒ–åçš„å‘é‡

### 3. åŠ è½½ç¤ºä¾‹

```python
import numpy as np

# åŠ è½½embeddings
embeddings = np.load('dataset1_labeled_embeddings.npy')
labels = np.load('dataset1_labeled_labels.npy')

print(f"Embeddings shape: {embeddings.shape}")  # (N, 768)
print(f"Labels shape: {labels.shape}")          # (N,)

# éªŒè¯å½’ä¸€åŒ–
norms = np.linalg.norm(embeddings, axis=1)
print(f"Vector norms (should be ~1.0): {norms[:5]}")
```

## ğŸ”— ä¸ETMæ¨¡å‹çš„æ¥å£

### 1. æ•°æ®æ ¼å¼è¦æ±‚

ä¸ºç¡®ä¿ETMæ¨¡å‹èƒ½å¤Ÿé¡ºåˆ©æ¥æ”¶embeddingè¾“å‡ºï¼Œéœ€æ»¡è¶³ï¼š

```python
# ETMæœŸæœ›çš„è¾“å…¥æ ¼å¼
{
    'embeddings': np.ndarray,  # Shape: (num_docs, embedding_dim)
    'vocabulary_size': int,     # è¯æ±‡è¡¨å¤§å°
    'labels': np.ndarray,       # Shape: (num_docs,), å¯é€‰
    'doc_ids': List[str]        # æ–‡æ¡£IDåˆ—è¡¨
}
```

### 2. æ¥å£è„šæœ¬

åœ¨ `/root/autodl-tmp/embedding/scripts/prepare_for_etm.py` åˆ›å»ºï¼š

```python
import numpy as np
import json
import os

def prepare_etm_input(
    embeddings_path: str,
    labels_path: str = None,
    output_dir: str = "/root/autodl-tmp/ETM/inputs"
):
    """
    å‡†å¤‡ETMè¾“å…¥æ•°æ®
    
    Args:
        embeddings_path: embeddingsçš„.npyæ–‡ä»¶è·¯å¾„
        labels_path: æ ‡ç­¾çš„.npyæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        output_dir: è¾“å‡ºç›®å½•
    """
    # åŠ è½½embeddings
    embeddings = np.load(embeddings_path)
    
    # åŠ è½½æ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
    labels = np.load(labels_path) if labels_path and os.path.exists(labels_path) else None
    
    # åˆ›å»ºè¾“å‡º
    os.makedirs(output_dir, exist_ok=True)
    
    dataset_name = os.path.basename(embeddings_path).replace('_embeddings.npy', '')
    
    # ä¿å­˜ä¸ºETMæ ¼å¼
    etm_data = {
        'embeddings': embeddings.tolist(),
        'num_docs': embeddings.shape[0],
        'embedding_dim': embeddings.shape[1],
        'labels': labels.tolist() if labels is not None else None,
        'doc_ids': [f"doc_{i}" for i in range(embeddings.shape[0])]
    }
    
    output_path = os.path.join(output_dir, f"{dataset_name}_etm_input.json")
    with open(output_path, 'w') as f:
        json.dump(etm_data, f)
    
    print(f"âœ“ Prepared ETM input: {output_path}")
    print(f"  - Documents: {embeddings.shape[0]}")
    print(f"  - Embedding dim: {embeddings.shape[1]}")
    print(f"  - Has labels: {labels is not None}")
    
    return output_path

def main():
    """ä¸ºæ‰€æœ‰æ•°æ®é›†å‡†å¤‡ETMè¾“å…¥"""
    
    # Zero-shot embeddings
    zero_shot_dir = "/root/autodl-tmp/embedding/outputs/zero_shot"
    for filename in os.listdir(zero_shot_dir):
        if filename.endswith('_embeddings.npy'):
            embeddings_path = os.path.join(zero_shot_dir, filename)
            labels_path = embeddings_path.replace('_embeddings.npy', '_labels.npy')
            prepare_etm_input(embeddings_path, labels_path)
    
    # Supervised embeddings
    supervised_dir = "/root/autodl-tmp/embedding/outputs/supervised"
    for filename in os.listdir(supervised_dir):
        if filename.endswith('_lora_embeddings.npy'):
            embeddings_path = os.path.join(supervised_dir, filename)
            labels_path = os.path.join(supervised_dir, filename.replace('_lora_embeddings.npy', '_labels.npy'))
            prepare_etm_input(embeddings_path, labels_path)
    
    # Unsupervised embeddings
    unsupervised_dir = "/root/autodl-tmp/embedding/outputs/unsupervised"
    for filename in os.listdir(unsupervised_dir):
        if filename.endswith('_unsupervised_embeddings.npy'):
            embeddings_path = os.path.join(unsupervised_dir, filename)
            prepare_etm_input(embeddings_path)

if __name__ == "__main__":
    main()
```

## ğŸ“Š è´¨é‡æ£€æŸ¥

### éªŒè¯è„šæœ¬

åœ¨ `/root/autodl-tmp/embedding/scripts/validate_embeddings.py` åˆ›å»ºï¼š

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import os

def validate_embeddings(embeddings_path: str):
    """éªŒè¯embeddingsè´¨é‡"""
    
    print(f"\n{'='*60}")
    print(f"Validating: {os.path.basename(embeddings_path)}")
    print(f"{'='*60}")
    
    # åŠ è½½
    embeddings = np.load(embeddings_path)
    
    # 1. åŸºæœ¬ç»Ÿè®¡
    print(f"\n1. Basic Statistics:")
    print(f"   Shape: {embeddings.shape}")
    print(f"   Mean: {embeddings.mean():.4f}")
    print(f"   Std: {embeddings.std():.4f}")
    print(f"   Min: {embeddings.min():.4f}")
    print(f"   Max: {embeddings.max():.4f}")
    
    # 2. å½’ä¸€åŒ–æ£€æŸ¥
    norms = np.linalg.norm(embeddings, axis=1)
    print(f"\n2. Normalization Check:")
    print(f"   Mean norm: {norms.mean():.4f} (should be ~1.0)")
    print(f"   Std norm: {norms.std():.4f}")
    
    # 3. è¯­ä¹‰ç›¸ä¼¼æ€§æ£€æŸ¥ï¼ˆéšæœºé‡‡æ ·ï¼‰
    if len(embeddings) > 10:
        sample_indices = np.random.choice(len(embeddings), min(10, len(embeddings)), replace=False)
        sample_embs = embeddings[sample_indices]
        sim_matrix = cosine_similarity(sample_embs)
        
        print(f"\n3. Semantic Similarity (sample):")
        print(f"   Average pairwise similarity: {sim_matrix[np.triu_indices_from(sim_matrix, k=1)].mean():.4f}")
        print(f"   Min similarity: {sim_matrix[np.triu_indices_from(sim_matrix, k=1)].min():.4f}")
        print(f"   Max similarity: {sim_matrix[np.triu_indices_from(sim_matrix, k=1)].max():.4f}")
    
    # 4. æ£€æŸ¥NaNå’ŒInf
    has_nan = np.isnan(embeddings).any()
    has_inf = np.isinf(embeddings).any()
    print(f"\n4. Data Quality:")
    print(f"   Has NaN: {has_nan}")
    print(f"   Has Inf: {has_inf}")
    
    if has_nan or has_inf:
        print("   âš ï¸  WARNING: Contains NaN or Inf values!")
    else:
        print("   âœ“ Clean data")
    
    return not (has_nan or has_inf)

def main():
    """éªŒè¯æ‰€æœ‰embeddings"""
    
    directories = [
        "/root/autodl-tmp/embedding/outputs/zero_shot",
        "/root/autodl-tmp/embedding/outputs/supervised",
        "/root/autodl-tmp/embedding/outputs/unsupervised"
    ]
    
    all_valid = True
    for directory in directories:
        if not os.path.exists(directory):
            continue
            
        for filename in os.listdir(directory):
            if filename.endswith('_embeddings.npy'):
                filepath = os.path.join(directory, filename)
                is_valid = validate_embeddings(filepath)
                all_valid = all_valid and is_valid
    
    print(f"\n{'='*60}")
    if all_valid:
        print("âœ… All embeddings are valid!")
    else:
        print("âŒ Some embeddings have quality issues")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()
```

```bash
python validate_embeddings.py
```

## ğŸ¯ å®Œæ•´å·¥ä½œæµ

### ä¸€é”®è¿è¡Œè„šæœ¬

åœ¨ `/root/autodl-tmp/embedding/run_all.sh` åˆ›å»ºï¼š

```bash
#!/bin/bash

echo "=========================================="
echo "Qwen3-Embedding Training Pipeline"
echo "=========================================="

cd /root/autodl-tmp/embedding/scripts

# Step 1: Zero-shot
echo -e "\n[Step 1/5] Running Zero-shot embedding..."
python zero_shot_embedding.py

# Step 2: Prepare supervised data
echo -e "\n[Step 2/5] Preparing supervised training data..."
python prepare_supervised_data.py

# Step 3: Train supervised models
echo -e "\n[Step 3/5] Training supervised models with LoRA..."
python train_supervised_lora.py

# Step 4: Generate supervised embeddings
echo -e "\n[Step 4/5] Generating supervised embeddings..."
python generate_supervised_embeddings.py

# Step 5: Train unsupervised models
echo -e "\n[Step 5/5] Training unsupervised models..."
python train_unsupervised.py
python generate_unsupervised_embeddings.py

# Validation
echo -e "\n[Validation] Checking embedding quality..."
python validate_embeddings.py

# Prepare for ETM
echo -e "\n[ETM Preparation] Preparing data for ETM model..."
python prepare_for_etm.py

echo -e "\n=========================================="
echo "âœ… All tasks completed!"
echo "=========================================="
```

```bash
chmod +x /root/autodl-tmp/embedding/run_all.sh
./run_all.sh
```

## ğŸ“ ç›‘æ§è®­ç»ƒ

### TensorBoardå¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰

```bash
# å®‰è£…tensorboard
pip install tensorboard

# å¯åŠ¨tensorboard
tensorboard --logdir=/root/autodl-tmp/embedding/logs --port=6006
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ˜¾å­˜ç®¡ç†**: 
   - å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå‡å° `batch_size`
   - è€ƒè™‘ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯: `accumulation_steps=4`

2. **æ•°æ®æ ¼å¼**:
   - ç¡®ä¿ `data_loader.py` ä¸­çš„æ•°æ®åŠ è½½é€»è¾‘ä¸å®é™…æ•°æ®æ ¼å¼åŒ¹é…
   - æ£€æŸ¥æ–‡æœ¬ç¼–ç ï¼ˆUTF-8ï¼‰

3. **LoRAå‚æ•°**:
   - `r=8`: ä½ç§©åˆ†è§£çš„ç§©ï¼Œè¶Šå¤§å‚æ•°è¶Šå¤š
   - `lora_alpha=32`: ç¼©æ”¾å› å­ï¼Œæ§åˆ¶LoRAçš„å½±å“å¼ºåº¦

4. **è®­ç»ƒæ—¶é—´**:
   - Zero-shot: å³æ—¶å®Œæˆ
   - æœ‰ç›‘ç£è®­ç»ƒ: æ¯ä¸ªæ•°æ®é›†çº¦1-3å°æ—¶
   - æ— ç›‘ç£è®­ç»ƒ: æ¯ä¸ªæ•°æ®é›†çº¦2-4å°æ—¶

5. **æ£€æŸ¥ç‚¹ä¿å­˜**:
   - æ‰€æœ‰æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜åœ¨ `/checkpoints/` ç›®å½•
   - å¯ä»¥éšæ—¶ä¸­æ–­å¹¶ä»æ£€æŸ¥ç‚¹æ¢å¤

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDA out of memory**:
   ```python
   # å‡å°batch_size
   batch_size = 8  # ä»16é™åˆ°8
   ```

2. **æ¨¡å‹ä¸‹è½½å¤±è´¥**:
   ```bash
   # ä½¿ç”¨é•œåƒæº
   export HF_ENDPOINT=https://hf-mirror.com
   ```

3. **æ•°æ®åŠ è½½é”™è¯¯**:
   ```python
   # æ£€æŸ¥æ•°æ®è·¯å¾„
   ls -la /root/autodl-tmp/data/
   ```

## ğŸ“– å‚è€ƒèµ„æ–™

- Qwen3-Embeddingè®ºæ–‡: [arXiv:2506.05176](https://arxiv.org/abs/2506.05176)
- LoRAè®ºæ–‡: [arXiv:2106.09685](https://arxiv.org/abs/2106.09685)
- MTEB Benchmark: [https://github.com/embeddings-benchmark/mteb](https://github.com/embeddings-benchmark/mteb)

---

**ä¸‹ä¸€æ­¥**: å°†ç”Ÿæˆçš„embeddingsè¾“å…¥åˆ°ETMæ¨¡å‹è¿›è¡Œä¸»é¢˜å»ºæ¨¡è®­ç»ƒï¼Œæˆ‘ä»¬åªè¿›è¡Œåˆ°embeddingsè¿™ä¸€æ­¥ï¼Œ