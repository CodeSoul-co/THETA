"""
Result Interpreter Agent
Transforms technical metrics into business insights.

Responsibilities:
1. Load analysis results (metrics, topics, theta, beta)
2. Call LLM to generate business-friendly interpretations
3. Answer user questions about results
4. Generate analysis summaries

Core Functions:
- interpret_metrics(): Interpret evaluation metrics
- interpret_topics(): Interpret topic content
- generate_summary(): Generate analysis summary
- answer_question(): Answer user questions
"""

import os
import json
import logging
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime

from ..config.llm_config import LLMConfigManager, LLMConfig


class ResultInterpreterAgent:
    """
    Result Interpreter Agent
    
    Transforms topic model technical outputs into business-understandable insights.
    """
    
    # Metric interpretation templates
    METRIC_TEMPLATES = {
        "topic_coherence_npmi_avg": {
            "name": "ä¸»é¢˜è¿è´¯æ€§ (NPMI)",
            "name_en": "Topic Coherence (NPMI)",
            "range": "[-1, 1]",
            "good_threshold": 0.1,
            "excellent_threshold": 0.2,
            "interpretation": {
                "zh": "è¡¡é‡ä¸»é¢˜å†…è¯è¯­çš„è¯­ä¹‰å…³è”ç¨‹åº¦ã€‚å€¼è¶Šé«˜è¡¨ç¤ºä¸»é¢˜å†…çš„è¯è¯­åœ¨åŸå§‹æ–‡æ¡£ä¸­å…±ç°é¢‘ç‡è¶Šé«˜ï¼Œä¸»é¢˜è¶Šè¿è´¯ã€‚",
                "en": "Measures semantic association between words within a topic. Higher values indicate more coherent topics."
            },
            "business_meaning": {
                "zh": "é«˜è¿è´¯æ€§æ„å‘³ç€è¯†åˆ«å‡ºçš„ä¸»é¢˜æ›´æœ‰æ„ä¹‰ï¼Œè¯è¯­ä¹‹é—´æœ‰æ˜ç¡®çš„è¯­ä¹‰è”ç³»ã€‚",
                "en": "High coherence means identified topics are more meaningful with clear semantic connections."
            }
        },
        "topic_coherence_cv_avg": {
            "name": "ä¸»é¢˜è¿è´¯æ€§ (C_V)",
            "name_en": "Topic Coherence (C_V)",
            "range": "[0, 1]",
            "good_threshold": 0.4,
            "excellent_threshold": 0.6,
            "interpretation": {
                "zh": "åŸºäºæ»‘åŠ¨çª—å£çš„è¿è´¯æ€§æŒ‡æ ‡ï¼Œè€ƒè™‘è¯è¯­åœ¨æ–‡æ¡£ä¸­çš„ä¸Šä¸‹æ–‡å…±ç°ã€‚",
                "en": "Sliding window based coherence metric considering contextual co-occurrence."
            },
            "business_meaning": {
                "zh": "åæ˜ ä¸»é¢˜è¯åœ¨å®é™…æ–‡æœ¬ä¸­çš„è¯­å¢ƒå…³è”å¼ºåº¦ã€‚",
                "en": "Reflects contextual association strength of topic words in actual text."
            }
        },
        "topic_coherence_umass_avg": {
            "name": "ä¸»é¢˜è¿è´¯æ€§ (UMass)",
            "name_en": "Topic Coherence (UMass)",
            "range": "(-âˆ, 0]",
            "good_threshold": -2.0,
            "excellent_threshold": -1.0,
            "interpretation": {
                "zh": "åŸºäºæ–‡æ¡£å†…è¯è¯­å…±ç°çš„è¿è´¯æ€§æŒ‡æ ‡ï¼Œå€¼è¶Šæ¥è¿‘0è¶Šå¥½ã€‚",
                "en": "Document-based co-occurrence coherence. Values closer to 0 are better."
            },
            "business_meaning": {
                "zh": "è¡¡é‡ä¸»é¢˜è¯åœ¨åŒä¸€æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡ã€‚",
                "en": "Measures how often topic words appear together in the same document."
            }
        },
        "topic_diversity_td": {
            "name": "ä¸»é¢˜å¤šæ ·æ€§ (TD)",
            "name_en": "Topic Diversity (TD)",
            "range": "[0, 1]",
            "good_threshold": 0.7,
            "excellent_threshold": 0.85,
            "interpretation": {
                "zh": "è¡¡é‡ä¸åŒä¸»é¢˜ä¹‹é—´çš„åŒºåˆ†åº¦ã€‚å€¼è¶Šé«˜è¡¨ç¤ºä¸»é¢˜ä¹‹é—´é‡å è¯è¶Šå°‘ï¼ŒåŒºåˆ†åº¦è¶Šé«˜ã€‚",
                "en": "Measures distinction between topics. Higher values indicate less word overlap."
            },
            "business_meaning": {
                "zh": "é«˜å¤šæ ·æ€§æ„å‘³ç€æ¨¡å‹è¯†åˆ«å‡ºäº†ä¸åŒçš„è®¨è®ºé¢†åŸŸï¼Œè€Œéé‡å¤çš„ä¸»é¢˜ã€‚",
                "en": "High diversity means the model identified distinct discussion areas, not repetitive topics."
            }
        },
        "topic_diversity_irbo": {
            "name": "ä¸»é¢˜å¤šæ ·æ€§ (iRBO)",
            "name_en": "Topic Diversity (iRBO)",
            "range": "[0, 1]",
            "good_threshold": 0.7,
            "excellent_threshold": 0.85,
            "interpretation": {
                "zh": "åŸºäºæ’åçš„ä¸»é¢˜å¤šæ ·æ€§æŒ‡æ ‡ï¼Œè€ƒè™‘è¯è¯­æ’åé¡ºåºçš„å·®å¼‚ã€‚",
                "en": "Rank-based diversity metric considering word ranking order differences."
            },
            "business_meaning": {
                "zh": "åæ˜ ä¸»é¢˜ä¹‹é—´ä¸ä»…è¯è¯­ä¸åŒï¼Œè€Œä¸”é‡è¦æ€§æ’åºä¹Ÿä¸åŒã€‚",
                "en": "Reflects that topics differ not only in words but also in importance ranking."
            }
        },
        "topic_exclusivity_avg": {
            "name": "ä¸»é¢˜æ’ä»–æ€§",
            "name_en": "Topic Exclusivity",
            "range": "[0, 1]",
            "good_threshold": 0.3,
            "excellent_threshold": 0.5,
            "interpretation": {
                "zh": "è¡¡é‡æ¯ä¸ªä¸»é¢˜çš„ç‰¹å¾è¯æ˜¯å¦ä¸“å±äºè¯¥ä¸»é¢˜ã€‚å€¼è¶Šé«˜è¡¨ç¤ºä¸»é¢˜ç‰¹å¾è¶Šé²œæ˜ã€‚",
                "en": "Measures whether topic words are exclusive to that topic. Higher values indicate more distinctive topics."
            },
            "business_meaning": {
                "zh": "é«˜æ’ä»–æ€§æ„å‘³ç€æ¯ä¸ªä¸»é¢˜éƒ½æœ‰ç‹¬ç‰¹çš„æ ‡å¿—æ€§è¯è¯­ï¼Œä¾¿äºç†è§£å’Œå‘½åã€‚",
                "en": "High exclusivity means each topic has unique signature words, easier to understand and name."
            }
        },
        "topic_significance_avg": {
            "name": "ä¸»é¢˜æ˜¾è‘—æ€§",
            "name_en": "Topic Significance",
            "range": "[0, 1]",
            "good_threshold": 0.03,
            "excellent_threshold": 0.05,
            "interpretation": {
                "zh": "è¡¡é‡æ¯ä¸ªä¸»é¢˜åœ¨æ–‡æ¡£é›†ä¸­çš„é‡è¦ç¨‹åº¦ã€‚å€¼è¶Šé«˜è¡¨ç¤ºè¯¥ä¸»é¢˜è¦†ç›–çš„æ–‡æ¡£è¶Šå¤šã€‚",
                "en": "Measures importance of each topic in the document collection."
            },
            "business_meaning": {
                "zh": "åæ˜ ä¸»é¢˜åœ¨æ•´ä½“è®¨è®ºä¸­çš„æƒé‡åˆ†å¸ƒã€‚",
                "en": "Reflects weight distribution of topics in overall discussion."
            }
        },
        "perplexity": {
            "name": "å›°æƒ‘åº¦",
            "name_en": "Perplexity",
            "range": "[1, +âˆ)",
            "good_threshold": 500,
            "excellent_threshold": 200,
            "interpretation": {
                "zh": "è¡¡é‡æ¨¡å‹å¯¹æ–‡æ¡£çš„é¢„æµ‹èƒ½åŠ›ã€‚å€¼è¶Šä½è¡¨ç¤ºæ¨¡å‹æ‹Ÿåˆè¶Šå¥½ã€‚",
                "en": "Measures model's prediction capability. Lower values indicate better fit."
            },
            "business_meaning": {
                "zh": "ä½å›°æƒ‘åº¦æ„å‘³ç€æ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°è§£é‡Šæ–‡æ¡£çš„è¯è¯­åˆ†å¸ƒã€‚",
                "en": "Low perplexity means the model explains document word distribution well."
            },
            "lower_is_better": True
        }
    }
    
    # è´¨é‡ç­‰çº§å®šä¹‰
    QUALITY_LEVELS = {
        "excellent": {"zh": "ä¼˜ç§€", "en": "Excellent", "emoji": "ğŸŒŸ"},
        "good": {"zh": "è‰¯å¥½", "en": "Good", "emoji": "âœ…"},
        "fair": {"zh": "ä¸€èˆ¬", "en": "Fair", "emoji": "âš ï¸"},
        "poor": {"zh": "è¾ƒå·®", "en": "Poor", "emoji": "âŒ"}
    }
    
    def __init__(
        self, 
        base_dir: str = "/root/autodl-tmp",
        llm_config: Optional[Dict] = None
    ):
        self.base_dir = Path(base_dir)
        self.logger = self._setup_logger()
        
        # åˆå§‹åŒ–LLMé…ç½®
        if llm_config:
            self.llm_config = LLMConfigManager.get_config(
                llm_config.get("provider", "qwen"),
                llm_config
            )
        else:
            self.llm_config = LLMConfigManager.get_qwen_config()
        
        # å¯¹è¯å†å²ï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰
        self.conversation_history: Dict[str, List[Dict]] = {}
    
    def _setup_logger(self) -> logging.Logger:
        logger = logging.getLogger(f"ResultInterpreterAgent_{id(self)}")
        logger.setLevel(logging.INFO)
        return logger
    
    def interpret_metrics(
        self, 
        job_id: str,
        language: str = "zh"
    ) -> Dict[str, Any]:
        """
        è§£è¯»è¯„ä¼°æŒ‡æ ‡
        
        Args:
            job_id: ä»»åŠ¡ID
            language: è¯­è¨€ (zh/en)
            
        Returns:
            åŒ…å«æŒ‡æ ‡è§£è¯»çš„å­—å…¸
        """
        try:
            # åŠ è½½æŒ‡æ ‡æ•°æ®
            metrics = self._load_metrics(job_id)
            
            interpretations = []
            overall_quality = {"excellent": 0, "good": 0, "fair": 0, "poor": 0}
            
            for metric_key, template in self.METRIC_TEMPLATES.items():
                if metric_key in metrics and metrics[metric_key] is not None:
                    value = metrics[metric_key]
                    
                    # è¯„ä¼°è´¨é‡ç­‰çº§
                    quality = self._evaluate_metric_quality(metric_key, value, template)
                    overall_quality[quality] += 1
                    
                    interpretation = {
                        "metric": metric_key,
                        "name": template["name"] if language == "zh" else template["name_en"],
                        "value": value,
                        "range": template["range"],
                        "quality": quality,
                        "quality_label": self.QUALITY_LEVELS[quality][language],
                        "quality_emoji": self.QUALITY_LEVELS[quality]["emoji"],
                        "interpretation": template["interpretation"][language],
                        "business_meaning": template["business_meaning"][language]
                    }
                    interpretations.append(interpretation)
            
            # ç”Ÿæˆæ€»ä½“è¯„ä¼°
            overall_assessment = self._generate_overall_assessment(
                overall_quality, language
            )
            
            return {
                "status": "success",
                "job_id": job_id,
                "metrics_count": len(interpretations),
                "interpretations": interpretations,
                "overall_quality": overall_quality,
                "overall_assessment": overall_assessment
            }
            
        except Exception as e:
            self.logger.error(f"Failed to interpret metrics: {str(e)}")
            return {
                "status": "failed",
                "job_id": job_id,
                "error": str(e)
            }
    
    def interpret_topics(
        self, 
        job_id: str,
        language: str = "zh",
        use_llm: bool = True
    ) -> Dict[str, Any]:
        """
        è§£è¯»ä¸»é¢˜å†…å®¹
        
        Args:
            job_id: ä»»åŠ¡ID
            language: è¯­è¨€ (zh/en)
            use_llm: æ˜¯å¦ä½¿ç”¨LLMç”Ÿæˆæ·±åº¦è§£è¯»
            
        Returns:
            åŒ…å«ä¸»é¢˜è§£è¯»çš„å­—å…¸
        """
        try:
            # åŠ è½½ä¸»é¢˜æ•°æ®
            topics = self._load_topics(job_id)
            analysis_result = self._load_analysis_result(job_id)
            
            topic_interpretations = []
            
            for topic in topics:
                topic_id = topic.get("id", topic.get("topic_id", 0))
                keywords = topic.get("keywords", topic.get("words", []))
                proportion = topic.get("proportion", 0)
                
                # åŸºç¡€è§£è¯»
                interpretation = {
                    "topic_id": topic_id,
                    "keywords": keywords[:10] if isinstance(keywords, list) else keywords,
                    "proportion": proportion,
                    "proportion_percent": f"{proportion * 100:.1f}%"
                }
                
                # ä½¿ç”¨LLMç”Ÿæˆè¯­ä¹‰è§£è¯»
                if use_llm and keywords:
                    semantic_interpretation = self._generate_topic_interpretation(
                        topic_id, keywords, language
                    )
                    interpretation["semantic_interpretation"] = semantic_interpretation
                
                topic_interpretations.append(interpretation)
            
            # æŒ‰æ¯”ä¾‹æ’åº
            topic_interpretations.sort(
                key=lambda x: x.get("proportion", 0), 
                reverse=True
            )
            
            return {
                "status": "success",
                "job_id": job_id,
                "topics_count": len(topic_interpretations),
                "topics": topic_interpretations
            }
            
        except Exception as e:
            self.logger.error(f"Failed to interpret topics: {str(e)}")
            return {
                "status": "failed",
                "job_id": job_id,
                "error": str(e)
            }
    
    def generate_summary(
        self, 
        job_id: str,
        language: str = "zh"
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆåˆ†ææ‘˜è¦
        
        Args:
            job_id: ä»»åŠ¡ID
            language: è¯­è¨€ (zh/en)
            
        Returns:
            åŒ…å«åˆ†ææ‘˜è¦çš„å­—å…¸
        """
        try:
            # åŠ è½½æ‰€æœ‰æ•°æ®
            metrics = self._load_metrics(job_id)
            topics = self._load_topics(job_id)
            analysis_result = self._load_analysis_result(job_id)
            
            # å‡†å¤‡ä¸Šä¸‹æ–‡
            context = self._prepare_summary_context(metrics, topics, analysis_result)
            
            # ä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦
            summary = self._generate_llm_summary(context, language)
            
            return {
                "status": "success",
                "job_id": job_id,
                "summary": summary,
                "generated_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"Failed to generate summary: {str(e)}")
            return {
                "status": "failed",
                "job_id": job_id,
                "error": str(e)
            }
    
    def answer_question(
        self, 
        job_id: str,
        question: str,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å›ç­”ç”¨æˆ·é—®é¢˜ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰
        
        Args:
            job_id: ä»»åŠ¡ID
            question: ç”¨æˆ·é—®é¢˜
            session_id: ä¼šè¯IDï¼ˆç”¨äºå¤šè½®å¯¹è¯ï¼‰
            
        Returns:
            åŒ…å«å›ç­”çš„å­—å…¸
        """
        try:
            # åŠ è½½åˆ†ææ•°æ®
            metrics = self._load_metrics(job_id)
            topics = self._load_topics(job_id)
            analysis_result = self._load_analysis_result(job_id)
            
            # å‡†å¤‡ä¸Šä¸‹æ–‡
            context = self._prepare_qa_context(metrics, topics, analysis_result)
            
            # è·å–å¯¹è¯å†å²
            if session_id:
                history = self.conversation_history.get(session_id, [])
            else:
                session_id = f"{job_id}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
                history = []
            
            # ç”Ÿæˆå›ç­”
            answer = self._generate_qa_answer(question, context, history)
            
            # æ›´æ–°å¯¹è¯å†å²
            history.append({"role": "user", "content": question})
            history.append({"role": "assistant", "content": answer})
            self.conversation_history[session_id] = history[-10:]  # ä¿ç•™æœ€è¿‘10è½®
            
            return {
                "status": "success",
                "job_id": job_id,
                "session_id": session_id,
                "question": question,
                "answer": answer
            }
            
        except Exception as e:
            self.logger.error(f"Failed to answer question: {str(e)}")
            return {
                "status": "failed",
                "job_id": job_id,
                "question": question,
                "error": str(e)
            }
    
    def _evaluate_metric_quality(
        self, 
        metric_key: str, 
        value: float,
        template: Dict
    ) -> str:
        """è¯„ä¼°æŒ‡æ ‡è´¨é‡ç­‰çº§"""
        good_threshold = template.get("good_threshold", 0)
        excellent_threshold = template.get("excellent_threshold", 0)
        lower_is_better = template.get("lower_is_better", False)
        
        if lower_is_better:
            if value <= excellent_threshold:
                return "excellent"
            elif value <= good_threshold:
                return "good"
            elif value <= good_threshold * 2:
                return "fair"
            else:
                return "poor"
        else:
            if value >= excellent_threshold:
                return "excellent"
            elif value >= good_threshold:
                return "good"
            elif value >= good_threshold * 0.5:
                return "fair"
            else:
                return "poor"
    
    def _generate_overall_assessment(
        self, 
        quality_counts: Dict[str, int],
        language: str
    ) -> str:
        """ç”Ÿæˆæ€»ä½“è¯„ä¼°"""
        total = sum(quality_counts.values())
        if total == 0:
            return "æ— æ³•è¯„ä¼°" if language == "zh" else "Unable to assess"
        
        excellent_ratio = quality_counts["excellent"] / total
        good_ratio = (quality_counts["excellent"] + quality_counts["good"]) / total
        
        if excellent_ratio >= 0.5:
            if language == "zh":
                return "ğŸŒŸ æ•´ä½“è¡¨ç°ä¼˜ç§€ï¼Œä¸»é¢˜æ¨¡å‹è´¨é‡å¾ˆé«˜ï¼Œç»“æœå¯ä¿¡åº¦å¼ºã€‚"
            return "ğŸŒŸ Excellent overall performance. High quality topic model with strong reliability."
        elif good_ratio >= 0.6:
            if language == "zh":
                return "âœ… æ•´ä½“è¡¨ç°è‰¯å¥½ï¼Œä¸»é¢˜æ¨¡å‹è´¨é‡è¾ƒå¥½ï¼Œç»“æœå…·æœ‰å‚è€ƒä»·å€¼ã€‚"
            return "âœ… Good overall performance. Quality topic model with valuable results."
        elif good_ratio >= 0.4:
            if language == "zh":
                return "âš ï¸ æ•´ä½“è¡¨ç°ä¸€èˆ¬ï¼Œå»ºè®®è°ƒæ•´å‚æ•°æˆ–å¢åŠ æ•°æ®é‡ä»¥æå‡è´¨é‡ã€‚"
            return "âš ï¸ Fair overall performance. Consider adjusting parameters or increasing data."
        else:
            if language == "zh":
                return "âŒ æ•´ä½“è¡¨ç°è¾ƒå·®ï¼Œå»ºè®®é‡æ–°è°ƒæ•´æ¨¡å‹å‚æ•°æˆ–æ£€æŸ¥æ•°æ®è´¨é‡ã€‚"
            return "âŒ Poor overall performance. Recommend adjusting model parameters or checking data quality."
    
    def _generate_topic_interpretation(
        self, 
        topic_id: int,
        keywords: List,
        language: str
    ) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆä¸»é¢˜è¯­ä¹‰è§£è¯»"""
        # å¤„ç†keywordsæ ¼å¼
        if isinstance(keywords, list) and len(keywords) > 0:
            if isinstance(keywords[0], dict):
                keyword_str = ", ".join([k.get("word", str(k)) for k in keywords[:10]])
            else:
                keyword_str = ", ".join([str(k) for k in keywords[:10]])
        else:
            keyword_str = str(keywords)
        
        if language == "zh":
            prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¸»é¢˜å…³é”®è¯ï¼Œç”¨ä¸€å¥è¯æ¦‚æ‹¬è¿™ä¸ªä¸»é¢˜è®¨è®ºçš„å†…å®¹é¢†åŸŸï¼š

ä¸»é¢˜{topic_id}çš„å…³é”®è¯ï¼š{keyword_str}

è¦æ±‚ï¼š
1. ç›´æ¥æè¿°ä¸»é¢˜å†…å®¹ï¼Œä¸è¦è¯´"è¿™ä¸ªä¸»é¢˜æ˜¯å…³äº..."
2. ä½¿ç”¨ä¸šåŠ¡åŒ–è¯­è¨€ï¼Œé¿å…æŠ€æœ¯æœ¯è¯­
3. æ§åˆ¶åœ¨30å­—ä»¥å†…"""
        else:
            prompt = f"""Based on the following topic keywords, summarize the content area in one sentence:

Topic {topic_id} keywords: {keyword_str}

Requirements:
1. Directly describe the topic content
2. Use business language, avoid technical terms
3. Keep it under 30 words"""
        
        try:
            return self._call_llm(prompt)
        except Exception as e:
            self.logger.warning(f"LLM call failed for topic interpretation: {e}")
            return f"ä¸»é¢˜{topic_id}ç›¸å…³å†…å®¹" if language == "zh" else f"Topic {topic_id} related content"
    
    def _prepare_summary_context(
        self, 
        metrics: Dict,
        topics: List,
        analysis_result: Dict
    ) -> str:
        """å‡†å¤‡æ‘˜è¦ç”Ÿæˆçš„ä¸Šä¸‹æ–‡"""
        parts = []
        
        # æ·»åŠ æŒ‡æ ‡ä¿¡æ¯
        parts.append("=== è¯„ä¼°æŒ‡æ ‡ ===")
        for key, value in metrics.items():
            if isinstance(value, (int, float)) and not key.endswith("_per_topic"):
                parts.append(f"{key}: {value:.4f}" if isinstance(value, float) else f"{key}: {value}")
        
        # æ·»åŠ ä¸»é¢˜ä¿¡æ¯
        parts.append("\n=== ä¸»é¢˜åˆ—è¡¨ ===")
        for topic in topics[:10]:  # æœ€å¤š10ä¸ªä¸»é¢˜
            topic_id = topic.get("id", topic.get("topic_id", 0))
            keywords = topic.get("keywords", topic.get("words", []))
            if isinstance(keywords, list) and len(keywords) > 0:
                if isinstance(keywords[0], dict):
                    kw_str = ", ".join([k.get("word", str(k)) for k in keywords[:5]])
                else:
                    kw_str = ", ".join([str(k) for k in keywords[:5]])
            else:
                kw_str = str(keywords)
            proportion = topic.get("proportion", 0)
            parts.append(f"ä¸»é¢˜{topic_id} ({proportion*100:.1f}%): {kw_str}")
        
        return "\n".join(parts)
    
    def _prepare_qa_context(
        self, 
        metrics: Dict,
        topics: List,
        analysis_result: Dict
    ) -> str:
        """å‡†å¤‡é—®ç­”çš„ä¸Šä¸‹æ–‡"""
        return self._prepare_summary_context(metrics, topics, analysis_result)
    
    def _generate_llm_summary(self, context: str, language: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦"""
        if language == "zh":
            prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¸»é¢˜æ¨¡å‹åˆ†æç»“æœï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„åˆ†ææ‘˜è¦ï¼š

{context}

è¦æ±‚ï¼š
1. æ¦‚è¿°ä¸»è¦å‘ç°ï¼ˆ2-3å¥è¯ï¼‰
2. æŒ‡å‡ºæœ€é‡è¦çš„ä¸»é¢˜åŠå…¶å«ä¹‰
3. ç»™å‡ºè´¨é‡è¯„ä¼°ç»“è®º
4. æä¾›1-2æ¡å»ºè®®
5. æ€»å­—æ•°æ§åˆ¶åœ¨200å­—ä»¥å†…"""
        else:
            prompt = f"""Based on the following topic model analysis results, generate a concise summary:

{context}

Requirements:
1. Overview of main findings (2-3 sentences)
2. Highlight the most important topics and their meanings
3. Quality assessment conclusion
4. 1-2 recommendations
5. Keep total under 200 words"""
        
        try:
            return self._call_llm(prompt)
        except Exception as e:
            self.logger.error(f"LLM summary generation failed: {e}")
            return "æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥LLMé…ç½®ã€‚" if language == "zh" else "Summary generation failed. Please check LLM configuration."
    
    def _generate_qa_answer(
        self, 
        question: str,
        context: str,
        history: List[Dict]
    ) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆé—®ç­”å›ç­”"""
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸»é¢˜æ¨¡å‹åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¸®åŠ©ç”¨æˆ·ç†è§£ä¸»é¢˜æ¨¡å‹çš„åˆ†æç»“æœã€‚

åˆ†æåŸåˆ™ï¼š
- ä»å†…å®¹è¯­ä¹‰è§’åº¦è§£è¯»ï¼Œè€Œéç®—æ³•è§’åº¦
- ä½¿ç”¨ä¸šåŠ¡åŒ–è¯­è¨€ï¼Œé¿å…æŠ€æœ¯æœ¯è¯­
- ç»“åˆå…·ä½“æ•°æ®ç»™å‡ºè§£é‡Š
- å¦‚æœç”¨æˆ·é—®ä¸­æ–‡é—®é¢˜ï¼Œç”¨ä¸­æ–‡å›ç­”ï¼›è‹±æ–‡é—®é¢˜ç”¨è‹±æ–‡å›ç­”"""
        
        # æ„å»ºæ¶ˆæ¯
        messages = [{"role": "system", "content": system_prompt}]
        
        # æ·»åŠ å†å²å¯¹è¯
        for msg in history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯
            messages.append(msg)
        
        # æ·»åŠ å½“å‰é—®é¢˜
        user_message = f"åˆ†ææ•°æ®ï¼š\n{context}\n\nç”¨æˆ·é—®é¢˜ï¼š{question}"
        messages.append({"role": "user", "content": user_message})
        
        try:
            return self._call_llm_with_messages(messages)
        except Exception as e:
            self.logger.error(f"LLM QA generation failed: {e}")
            return f"å›ç­”ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLM APIï¼ˆå•è½®ï¼‰"""
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸»é¢˜æ¨¡å‹åˆ†æä¸“å®¶ã€‚"},
            {"role": "user", "content": prompt}
        ]
        return self._call_llm_with_messages(messages)
    
    def _call_llm_with_messages(self, messages: List[Dict]) -> str:
        """è°ƒç”¨LLM APIï¼ˆå¤šè½®ï¼‰"""
        import requests
        
        is_valid, error = LLMConfigManager.validate_config(self.llm_config)
        if not is_valid:
            return error
        
        try:
            # ä½¿ç”¨OpenAIå…¼å®¹æ¥å£
            from openai import OpenAI
            
            client = OpenAI(
                api_key=self.llm_config.api_key,
                base_url=self.llm_config.base_url
            )
            
            response = client.chat.completions.create(
                model=self.llm_config.model,
                messages=messages,
                temperature=self.llm_config.temperature,
                top_p=self.llm_config.top_p,
                max_tokens=self.llm_config.max_tokens
            )
            
            return response.choices[0].message.content.strip()
            
        except ImportError:
            # å›é€€åˆ°requests
            return self._call_llm_with_requests(messages)
        except Exception as e:
            self.logger.error(f"OpenAI client error: {e}")
            return self._call_llm_with_requests(messages)
    
    def _call_llm_with_requests(self, messages: List[Dict]) -> str:
        """ä½¿ç”¨requestsè°ƒç”¨LLM API"""
        import requests
        
        url = f"{self.llm_config.base_url.rstrip('/')}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.llm_config.api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.llm_config.model,
            "messages": messages,
            "temperature": self.llm_config.temperature,
            "top_p": self.llm_config.top_p,
            "max_tokens": self.llm_config.max_tokens
        }
        
        resp = requests.post(
            url, 
            headers=headers, 
            json=payload, 
            timeout=self.llm_config.timeout
        )
        
        if resp.status_code != 200:
            return f"LLM API error: HTTP {resp.status_code}"
        
        data = resp.json()
        return data.get("choices", [{}])[0].get("message", {}).get("content", "").strip()
    
    # ==================== æ•°æ®åŠ è½½æ–¹æ³• ====================
    
    def _load_metrics(self, job_id: str) -> Dict[str, Any]:
        """åŠ è½½æŒ‡æ ‡æ•°æ®"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            self.base_dir / "result" / job_id / "metrics.json",
            self.base_dir / "result" / job_id / "evaluation" / "metrics.json",
            self.base_dir / "result" / job_id / "analysis_result.json",
        ]
        
        for path in possible_paths:
            if path.exists():
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # å¦‚æœæ˜¯analysis_resultï¼Œæå–metricséƒ¨åˆ†
                    if "metrics" in data:
                        return data["metrics"]
                    return data
        
        return {}
    
    def _load_topics(self, job_id: str) -> List[Dict]:
        """åŠ è½½ä¸»é¢˜æ•°æ®"""
        possible_paths = [
            self.base_dir / "result" / job_id / "topics.json",
            self.base_dir / "result" / job_id / "topic_words.json",
            self.base_dir / "result" / job_id / "analysis_result.json",
        ]
        
        for path in possible_paths:
            if path.exists():
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if "topics" in data:
                        return data["topics"]
                    if isinstance(data, list):
                        return data
        
        return []
    
    def _load_analysis_result(self, job_id: str) -> Dict[str, Any]:
        """åŠ è½½å®Œæ•´åˆ†æç»“æœ"""
        path = self.base_dir / "result" / job_id / "analysis_result.json"
        if path.exists():
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
